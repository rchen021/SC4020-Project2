{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0198e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import trackintel as ti\n",
    "import gzip\n",
    "import os\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b202c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the .gz files\n",
    "data_files = {\n",
    "    'A': \"/home/aden/Desktop/Data_mining/Part_2/13237029/13237029/cityA_groundtruthdata.csv\",\n",
    "    'B': \"/home/aden/Desktop/Data_mining/Part_2/13237029/13237029/cityB_challengedata.csv\", \n",
    "    'C': \"/home/aden/Desktop/Data_mining/Part_2/13237029/13237029/cityC_challengedata.csv\", \n",
    "    'D': \"/home/aden/Desktop/Data_mining/Part_2/13237029/13237029/cityD_challengedata.csv\" \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf47eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import trackintel as ti\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import combinations\n",
    "from shapely.geometry import LineString\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Preprocessing: Load raw data, clean it, and save as a new CSV\n",
    "def preprocess_data(city):\n",
    "    \"\"\"\n",
    "    Preprocess the raw data for the specified city:\n",
    "    1. Load the data.\n",
    "    2. Remove invalid rows where x, y == -999.\n",
    "    3. Add a 'tracked_at' timestamp column.\n",
    "    4. Convert grid coordinates to approximate latitude/longitude.\n",
    "    5. Save the processed data to a new CSV.\n",
    "    \"\"\"\n",
    "    print(f\"Loading raw data for city {city}...\")\n",
    "    df = pd.read_csv(data_files[city])\n",
    "\n",
    "    # Drop rows with missing data (x, y marked as 999)\n",
    "    print(\"Cleaning data: Removing rows with invalid coordinates...\")\n",
    "    df = df[(df['x'] != 999) & (df['y'] != 999)]\n",
    "\n",
    "    # Add timestamp column\n",
    "    print(\"Adding timestamp data...\")\n",
    "    df['date'] = pd.to_datetime(df['d'], format='%j', errors='coerce')\n",
    "    df['time'] = pd.to_timedelta(df['t'] * 30, unit='m')\n",
    "    df['tracked_at'] = df['date'] + df['time']\n",
    "    df['tracked_at'] = df['tracked_at'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Convert grid coordinates to latitude/longitude\n",
    "    print(\"Converting grid coordinates to latitude and longitude...\")\n",
    "    lat0 = 35.0  # Starting latitude\n",
    "    lon0 = 135.0  # Starting longitude\n",
    "    delta_degree = 0.0045  # Approx. 500m in degrees\n",
    "    df['latitude'] = lat0 + (df['y'] - 1) * delta_degree\n",
    "    df['longitude'] = lon0 + (df['x'] - 1) * delta_degree\n",
    "\n",
    "    # Rename columns\n",
    "    print(\"Renaming columns for consistency...\")\n",
    "    df.rename(columns={'uid': 'user_id'}, inplace=True)\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    processed_file = f'data_{city}_preprocessed.csv'\n",
    "    df.to_csv(processed_file, index=False)\n",
    "    print(f\"Data for city {city} saved to {processed_file}\")\n",
    "\n",
    "# Generate triplegs from preprocessed data\n",
    "def gen_triplegs(city):\n",
    "    \"\"\"\n",
    "    Generate triplegs for the specified city:\n",
    "    1. Preprocess raw data.\n",
    "    2. Generate staypoints using positionfixes.\n",
    "    3. Generate triplegs between staypoints.\n",
    "    4. Save triplegs to a CSV.\n",
    "    \"\"\"\n",
    "    preprocess_data(city)\n",
    "\n",
    "    print(f\"Loading preprocessed data for city {city}...\")\n",
    "    # Load preprocessed data\n",
    "    pfs = ti.io.file.read_positionfixes_csv(\n",
    "        f'data_{city}_preprocessed.csv',\n",
    "        sep=',',\n",
    "        tz='utc',\n",
    "        index_col=None,\n",
    "        columns={\n",
    "            'user_id': 'user_id',\n",
    "            'tracked_at': 'tracked_at',\n",
    "            'longitude': 'longitude',\n",
    "            'latitude': 'latitude'\n",
    "        },\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "\n",
    "    # Generate staypoints\n",
    "    print(\"Generating staypoints from positionfixes...\")\n",
    "    pfs, sp = pfs.as_positionfixes.generate_staypoints(\n",
    "        method='sliding',\n",
    "        dist_threshold=0.5,  # Distance threshold (0.5 km)\n",
    "        time_threshold=900,  # Time threshold (15 minutes)\n",
    "        gap_threshold=300,\n",
    "        distance_metric='haversine',\n",
    "        include_last=True,\n",
    "        print_progress=True,\n",
    "        exclude_duplicate_pfs=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Generate triplegs\n",
    "    print(\"Generating triplegs from staypoints...\")\n",
    "    pfs, tpls = pfs.as_positionfixes.generate_triplegs(\n",
    "        staypoints=sp, method='between_staypoints', gap_threshold=90\n",
    "    )\n",
    "\n",
    "    # Save triplegs\n",
    "    print(\"Saving triplegs to CSV...\")\n",
    "    custom_write_triplegs_csv(tpls, f'triplegs_{city}.csv')\n",
    "\n",
    "# Save triplegs with geometries converted to WKT\n",
    "def custom_write_triplegs_csv(triplegs, filename):\n",
    "    \"\"\"\n",
    "    Save triplegs to a CSV file, ensuring geometries are converted to WKT format.\n",
    "    \"\"\"\n",
    "    print(\"Converting geometries to WKT format...\")\n",
    "    triplegs.loc[:, 'geom'] = triplegs['geom'].apply(\n",
    "        lambda x: x.wkt if x and isinstance(x, LineString) else None\n",
    "    )\n",
    "    triplegs.to_csv(filename)\n",
    "    print(f\"Triplegs saved to {filename}\")\n",
    "\n",
    "# Split long triplegs into smaller segments\n",
    "def split_long_triplegs(df, max_length=1000):\n",
    "    \"\"\"\n",
    "    Split triplegs with a large number of points into smaller segments.\n",
    "    \"\"\"\n",
    "    print(\"Splitting long triplegs into smaller segments...\")\n",
    "    new_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        geom = wkt.loads(row['geom'])\n",
    "        coords = list(geom.coords)\n",
    "        if len(coords) > max_length:\n",
    "            for i in range(0, len(coords), max_length):\n",
    "                new_geom = coords[i:i+max_length]\n",
    "                new_linestring = LineString(new_geom)\n",
    "                new_row = row.copy()  # Ensure new_row is an independent copy\n",
    "                new_row['geom'] = wkt.dumps(new_linestring)\n",
    "                new_rows.append(new_row)\n",
    "        else:\n",
    "            new_rows.append(row)\n",
    "    print(f\"Completed splitting triplegs. Total segments: {len(new_rows)}\")\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "def geom_to_grid_coords(geom):\n",
    "    coords = np.array(geom.coords)\n",
    "    # Convert latitude and longitude back to grid cell numbers\n",
    "    delta_degree = 0.0045\n",
    "    x_grid = ((coords[:, 0] - 135.0) / delta_degree + 1).astype(int)\n",
    "    y_grid = ((coords[:, 1] - 35.0) / delta_degree + 1).astype(int)\n",
    "    grid_coords = list(zip(x_grid, y_grid))  # Ensure this outputs tuples\n",
    "    return grid_coords\n",
    "\n",
    "\n",
    "def preprocess_triplegs(df, timestamp_col='started_at'):\n",
    "    print(\"Preprocessing triplegs for sequential pattern mining...\")\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "\n",
    "    # Filter data for the first 30 days\n",
    "    print(\"Filtering triplegs to include only the first 30 days...\")\n",
    "    start_date = df[timestamp_col].min()\n",
    "    end_date = start_date + pd.Timedelta(days=30)\n",
    "    df = df[(df[timestamp_col] >= start_date) & (df[timestamp_col] < end_date)]\n",
    "\n",
    "    # Parse 'geom' column into grid coordinates\n",
    "    print(\"Parsing geometries into grid coordinates...\")\n",
    "    df.loc[:, 'geom'] = df['geom'].apply(wkt.loads)\n",
    "    df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n",
    "\n",
    "    # Group sequences by user\n",
    "    user_sequences = df.groupby('user_id')['coords'].apply(list).tolist()\n",
    "\n",
    "    # Flatten the sequences and ensure tuples\n",
    "    sequences = []\n",
    "    for user_seq in user_sequences:\n",
    "        seq = []\n",
    "        for tripleg_coords in user_seq:\n",
    "            seq.extend(tripleg_coords)  # Extend the sequence\n",
    "        sequences.append(tuple(seq))  # Convert entire sequence to a tuple\n",
    "    return sequences\n",
    "\n",
    "\n",
    "# Generalized Sequential Pattern (GSP) mining\n",
    "def gsp(sequences, min_support):\n",
    "    \"\"\"\n",
    "    Implement the GSP algorithm to find frequent sequential patterns.\n",
    "    \"\"\"\n",
    "    print(\"Running GSP algorithm...\")\n",
    "    freq_seqs = {}  # Dictionary to store frequent sequences\n",
    "    length = 1  # Length of sequences\n",
    "    all_frequent_sequences = []  # List to store all frequent sequences\n",
    "\n",
    "    # Generate 1-sequences\n",
    "    print(\"Generating 1-sequences...\")\n",
    "    candidates = {}\n",
    "    for sequence in sequences:\n",
    "        for item in sequence:\n",
    "            candidates[item] = candidates.get(item, 0) + 1\n",
    "    freq_seqs[1] = { (item,): count for item, count in candidates.items() if count >= min_support }\n",
    "    all_frequent_sequences.extend(freq_seqs[1].keys())\n",
    "\n",
    "    # Generate longer sequences\n",
    "    length = 2\n",
    "    while freq_seqs.get(length - 1):\n",
    "        print(f\"Generating candidates for length {length}...\")\n",
    "        candidates = generate_candidates(freq_seqs[length - 1].keys(), length)\n",
    "        support_count = count_support_parallel(candidates, sequences)\n",
    "        freq_seqs[length] = prune_candidates(support_count, min_support)\n",
    "        all_frequent_sequences.extend(freq_seqs[length].keys())\n",
    "        print(f\"Frequent sequences of length {length}: {len(freq_seqs[length])}\")\n",
    "        length += 1\n",
    "    print(\"GSP algorithm completed.\")\n",
    "    return all_frequent_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6870d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def is_subsequence(candidate, sequence):\n",
    "    \"\"\"\n",
    "    Efficiently check if 'candidate' is a subsequence of 'sequence'.\n",
    "    \"\"\"\n",
    "    candidate_set = set(candidate)\n",
    "    sequence_set = set(sequence)\n",
    "    # Quick check: if candidate items aren't all in sequence, return False\n",
    "    if not candidate_set.issubset(sequence_set):\n",
    "        return False\n",
    "\n",
    "    # Use iterators for efficient traversal\n",
    "    seq_iter = iter(sequence)\n",
    "    return all(item in seq_iter for item in candidate)\n",
    "\n",
    "# Define 'single_candidate_support' at the top level\n",
    "def single_candidate_support(args):\n",
    "    \"\"\"\n",
    "    Worker function to count support for a single candidate.\n",
    "    \"\"\"\n",
    "    candidate, sequences = args\n",
    "    count = sum(is_subsequence(candidate, seq) for seq in sequences)\n",
    "    return candidate, count\n",
    "\n",
    "def count_support_parallel(candidates, sequences):\n",
    "    \"\"\"\n",
    "    Count support for candidates using multiprocessing.Pool and tqdm for progress bar support.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        print(\"No candidates to process!\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Counting support for {len(candidates)} candidates...\")\n",
    "\n",
    "    # Prepare arguments for worker processes\n",
    "    args = [(candidate, sequences) for candidate in candidates]\n",
    "\n",
    "    results = []\n",
    "    with Pool() as pool:\n",
    "        # Use tqdm to display progress\n",
    "        for result in tqdm(pool.imap_unordered(single_candidate_support, args),\n",
    "                           total=len(candidates), desc=\"Processing candidates\"):\n",
    "            results.append(result)\n",
    "\n",
    "    print(\"Support counting finished.\")\n",
    "    return dict(results)\n",
    "\n",
    "\n",
    "def generate_candidates(prev_freq_seqs, length):\n",
    "    \"\"\"\n",
    "    Generate candidate sequences of the specified length.\n",
    "    \n",
    "    Parameters:\n",
    "    - prev_freq_seqs: List of frequent sequences from the previous iteration.\n",
    "    - length: Length of sequences to generate.\n",
    "\n",
    "    Returns:\n",
    "    - candidates: A set of candidate sequences of the specified length.\n",
    "    \"\"\"\n",
    "    print(f\"Generating candidates for sequences of length {length}...\")\n",
    "    candidates = set()\n",
    "    prev_freq_seqs = list(prev_freq_seqs)\n",
    "    for i in range(len(prev_freq_seqs)):\n",
    "        for j in range(len(prev_freq_seqs)):\n",
    "            seq1 = prev_freq_seqs[i]\n",
    "            seq2 = prev_freq_seqs[j]\n",
    "            # Check if the last n-1 elements of seq1 match the first n-1 elements of seq2\n",
    "            if seq1[1:] == seq2[:-1]:\n",
    "                candidate = seq1 + (seq2[-1],)\n",
    "                candidates.add(candidate)\n",
    "    return candidates\n",
    "\n",
    "def filter_candidates_early(candidates, item_frequencies, min_support):\n",
    "    print(\"Filtering candidates early...\")\n",
    "    filtered = [\n",
    "        candidate for candidate in candidates\n",
    "        if all(item_frequencies.get(item, 0) >= min_support for item in candidate)\n",
    "    ]\n",
    "    print(f\"Filtered candidates from {len(candidates)} to {len(filtered)}\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def prune_candidates(support_count, min_support):\n",
    "    \"\"\"\n",
    "    Prune candidate sequences based on the minimum support threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - support_count: Dictionary of candidate sequences and their support counts.\n",
    "    - min_support: Minimum support threshold.\n",
    "\n",
    "    Returns:\n",
    "    - pruned_candidates: Dictionary of pruned candidates that meet the minimum support threshold.\n",
    "    \"\"\"\n",
    "    return {seq: count for seq, count in support_count.items() if count >= min_support}\n",
    "\n",
    "def is_subsequence(candidate, sequence):\n",
    "    \"\"\"\n",
    "    Efficiently check if 'candidate' is a subsequence of 'sequence'.\n",
    "    \"\"\"\n",
    "    seq_iter = iter(sequence)\n",
    "    return all(item in seq_iter for item in candidate)\n",
    "\n",
    "def save_gsp_results(gsp_results, output_file):\n",
    "    \"\"\"\n",
    "    Save the GSP results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - gsp_results: List of frequent sequences.\n",
    "    - output_file: Path to the output file.\n",
    "    \"\"\"\n",
    "    print(f\"Saving GSP results to {output_file}...\")\n",
    "    sequences_df = pd.DataFrame(gsp_results, columns=['Sequence'])\n",
    "    sequences_df.to_csv(output_file, index=False)\n",
    "    print(f\"GSP results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9af9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 13467512\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2900/1398267354.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minium support A= 49883\n",
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 3016752\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2900/1398267354.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minium support B= 12448\n",
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 2382997\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2900/1398267354.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minium support C= 9970\n",
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 632435\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2900/1398267354.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minium support D= 2991\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    for i in [\"A\",\"B\",\"C\",\"D\"]:\n",
    "        # Read triplegs data\n",
    "        df = pd.read_csv(f'triplegs_{i}.csv')\n",
    "\n",
    "        # Split long triplegs\n",
    "        df = split_long_triplegs(df)\n",
    "\n",
    "        # Preprocess triplegs to get sequences\n",
    "        sequences = preprocess_triplegs(df)\n",
    "\n",
    "        # Set minimum support\n",
    "        min_support = int(0.5 * len(sequences))  # Adjust as needed\n",
    "\n",
    "        print(f\"minium support {i}= {min_support}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "475dfc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 3016752\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2900/1398267354.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GSP algorithm...\n",
      "Generating 1-sequences...\n",
      "Generating candidates for length 2...\n",
      "Generating candidates for sequences of length 2...\n",
      "Counting support for 10404 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing candidates:   0%|          | 1/10404 [00:40<117:43:38, 40.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/data_mining/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m min_support \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequences))  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Run GSP algorithm\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m frequent_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mgsp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m     19\u001b[0m save_gsp_results(frequent_sequences, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequent_sequences_B.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 200\u001b[0m, in \u001b[0;36mgsp\u001b[0;34m(sequences, min_support)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating candidates for length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m candidates \u001b[38;5;241m=\u001b[39m generate_candidates(freq_seqs[length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys(), length)\n\u001b[0;32m--> 200\u001b[0m support_count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_support_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m freq_seqs[length] \u001b[38;5;241m=\u001b[39m prune_candidates(support_count, min_support)\n\u001b[1;32m    202\u001b[0m all_frequent_sequences\u001b[38;5;241m.\u001b[39mextend(freq_seqs[length]\u001b[38;5;241m.\u001b[39mkeys())\n",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m, in \u001b[0;36mcount_support_parallel\u001b[0;34m(candidates, sequences)\u001b[0m\n\u001b[1;32m     40\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool() \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Use tqdm to display progress\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m tqdm(pool\u001b[38;5;241m.\u001b[39mimap_unordered(single_candidate_support, args),\n\u001b[1;32m     44\u001b[0m                        total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(candidates), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing candidates\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupport counting finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/data_mining/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/envs/data_mining/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Read triplegs data\n",
    "    df = pd.read_csv('triplegs_B.csv')\n",
    "\n",
    "    # Split long triplegs\n",
    "    df = split_long_triplegs(df)\n",
    "\n",
    "    # Preprocess triplegs to get sequences\n",
    "    sequences = preprocess_triplegs(df)\n",
    "\n",
    "    # Set minimum support\n",
    "    min_support = int(0.5 * len(sequences))  # Adjust as needed\n",
    "\n",
    "    # Run GSP algorithm\n",
    "    frequent_sequences = gsp(sequences, min_support)\n",
    "\n",
    "    # Save results\n",
    "    save_gsp_results(frequent_sequences, 'frequent_sequences_B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2a410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 13467512\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176184/1398267354.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GSP algorithm...\n",
      "Generating 1-sequences...\n",
      "Generating candidates for length 2...\n",
      "Generating candidates for sequences of length 2...\n",
      "Counting support for 4 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing candidates: 100%|██████████| 4/4 [07:12<00:00, 108.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support counting finished.\n",
      "Frequent sequences of length 2: 0\n",
      "GSP algorithm completed.\n",
      "Saving GSP results to frequent_sequences_A.csv...\n",
      "GSP results saved to frequent_sequences_A.csv\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Read triplegs data\n",
    "    df = pd.read_csv('triplegs_A.csv')\n",
    "\n",
    "    # Split long triplegs\n",
    "    df = split_long_triplegs(df)\n",
    "\n",
    "    # Preprocess triplegs to get sequences\n",
    "    sequences = preprocess_triplegs(df)\n",
    "\n",
    "    # Set minimum support\n",
    "    min_support = int(0.5 * len(sequences))  # Adjust as needed\n",
    "\n",
    "    # Run GSP algorithm\n",
    "    frequent_sequences = gsp(sequences, min_support)\n",
    "\n",
    "    # Save results\n",
    "    save_gsp_results(frequent_sequences, 'frequent_sequences_A.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bc7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 13467512\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190575/1398267354.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minium support = 49883\n",
      "Running GSP algorithm...\n",
      "Generating 1-sequences...\n",
      "Generating candidates for length 2...\n",
      "Generating candidates for sequences of length 2...\n",
      "Counting support for 64 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing candidates:   8%|▊         | 5/64 [09:04<1:45:22, 107.16s/it]"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Read triplegs data\n",
    "    df = pd.read_csv('triplegs_A.csv')\n",
    "\n",
    "    # Split long triplegs\n",
    "    df = split_long_triplegs(df)\n",
    "\n",
    "    # Preprocess triplegs to get sequences\n",
    "    sequences = preprocess_triplegs(df)\n",
    "\n",
    "    # Set minimum support\n",
    "    min_support = int(0.5 * len(sequences))  # Adjust as needed\n",
    "    print(f\"minium support = {min_support}\")\n",
    "    # Run GSP algorithm\n",
    "    frequent_sequences = gsp(sequences, min_support)\n",
    "\n",
    "    # Save results\n",
    "    save_gsp_results(frequent_sequences, 'frequent_sequences_A.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f240a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Read triplegs data\n",
    "    df = pd.read_csv('triplegs_C.csv')\n",
    "\n",
    "    # Split long triplegs\n",
    "    df = split_long_triplegs(df)\n",
    "\n",
    "    # Preprocess triplegs to get sequences\n",
    "    sequences = preprocess_triplegs(df)\n",
    "\n",
    "    # Set minimum support\n",
    "    min_support = int(0.8 * len(sequences))  # Adjust as needed\n",
    "\n",
    "    # Run GSP algorithm\n",
    "    frequent_sequences = gsp(sequences, min_support)\n",
    "\n",
    "    # Save results\n",
    "    save_gsp_results(frequent_sequences, 'frequent_sequences_C.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e07918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c070c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 13467512\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_560926/1380636941.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GSP algorithm...\n",
      "Generating 1-sequences...\n",
      "Generating candidates for length 2...\n",
      "Generating candidates for sequences of length 2...\n",
      "Counting support for 64 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing candidates:   2%|▏         | 1/64 [02:05<2:11:21, 125.10s/it]"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Read triplegs data\n",
    "    df = pd.read_csv('triplegs_A.csv')\n",
    "\n",
    "    # Split long triplegs\n",
    "    df = split_long_triplegs(df)\n",
    "\n",
    "    # Preprocess triplegs to get sequences\n",
    "    sequences = preprocess_triplegs(df)\n",
    "\n",
    "    # Set minimum support\n",
    "    min_support = int(0.5 * len(sequences))  # Adjust as needed\n",
    "\n",
    "    # Run GSP algorithm\n",
    "    frequent_sequences = gsp(sequences, min_support)\n",
    "\n",
    "    # Save results\n",
    "    save_gsp_results(frequent_sequences, 'frequent_sequences_A.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1eb412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd546a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data for city B...\n",
      "Cleaning data: Removing rows with invalid coordinates...\n",
      "Adding timestamp data...\n",
      "Converting grid coordinates to latitude and longitude...\n",
      "Renaming columns for consistency...\n",
      "Data for city B saved to data_B_preprocessed.csv\n",
      "Loading preprocessed data for city B...\n",
      "Generating staypoints from positionfixes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:35<00:00, 709.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating triplegs from staypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aden/anaconda3/envs/data_mining/lib/python3.10/site-packages/trackintel/preprocessing/positionfixes.py:361: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  pfs[\"tripleg_id\"] = pfs[\"tripleg_id\"].ffill()\n",
      "/home/aden/anaconda3/envs/data_mining/lib/python3.10/site-packages/trackintel/preprocessing/positionfixes.py:573: UserWarning: The positionfixes with ids [     168      169      256 ... 23912055 23912826 23912827] lead to invalid tripleg geometries. The resulting triplegs were omitted and the tripleg id of the positionfixes was set to nan\n",
      "  warnings.warn(warn_string)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving triplegs to CSV...\n",
      "Converting geometries to WKT format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_487307/1380636941.py:110: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['LINESTRING (135.3555 35.45, 135.3555 35.4455, 135.3555 35.45, 135.3555 35.45)'\n",
      " 'LINESTRING (135.3645 35.4545, 135.4005 35.4815, 135.4005 35.4815, 135.4005 35.4815, 135.4005 35.4815, 135.4005 35.4815, 135.4005 35.4815, 135.4095 35.486, 135.4005 35.4815, 135.4005 35.4815, 135.4005 35.4815, 135.4005 35.486, 135.4005 35.4815, 135.4005 35.4815)'\n",
      " 'LINESTRING (135.3465 35.4365, 135.342 35.4275, 135.3465 35.441, 135.3555 35.45)'\n",
      " ... 'LINESTRING (135.387 35.6705, 135.387 35.666)'\n",
      " 'LINESTRING (135.3825 35.6795, 135.3645 35.675, 135.252 35.666, 135.225 35.648, 135.207 35.621, 135.207 35.621, 135.207 35.621, 135.2475 35.657, 135.3645 35.666, 135.387 35.666)'\n",
      " 'LINESTRING (135.387 35.666, 135.387 35.666, 135.0585 35.0495, 135.2295 35.198, 135.3915 35.6255)']' has dtype incompatible with geometry, please explicitly cast to a compatible dtype first.\n",
      "  triplegs.loc[:, 'geom'] = triplegs['geom'].apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplegs saved to triplegs_B.csv\n",
      "Splitting long triplegs into smaller segments...\n",
      "Completed splitting triplegs. Total segments: 3016752\n",
      "Preprocessing triplegs for sequential pattern mining...\n",
      "Filtering triplegs to include only the first 30 days...\n",
      "Parsing geometries into grid coordinates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_487307/1380636941.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'coords'] = df['geom'].apply(geom_to_grid_coords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GSP algorithm...\n",
      "Generating 1-sequences...\n",
      "Generating candidates for length 2...\n",
      "Generating candidates for sequences of length 2...\n",
      "Counting support for 961 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing candidates:   1%|▏         | 13/961 [05:09<6:13:18, 23.63s/it]"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    gen_triplegs(\"B\")\n",
    "    # Read triplegs data\n",
    "    df = pd.read_csv('triplegs_B.csv')\n",
    "\n",
    "    # Split long triplegs\n",
    "    df = split_long_triplegs(df)\n",
    "\n",
    "    # Preprocess triplegs to get sequences\n",
    "    sequences = preprocess_triplegs(df)\n",
    "\n",
    "    # Set minimum support\n",
    "    min_support = int(0.8 * len(sequences))  # Adjust as needed\n",
    "\n",
    "    # Run GSP algorithm\n",
    "    frequent_sequences = gsp(sequences, min_support)\n",
    "\n",
    "    # Save results\n",
    "    save_gsp_results(frequent_sequences, 'frequent_sequences_B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0a656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
