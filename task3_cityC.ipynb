{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Location Prediction - City C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We only consider the first 30 days (75 days in total) of the CityC dataset to reduce the data processing load. We believe this will not affect the result of analysis because the first 30 days is long enough capture the significant trends and patterns that are relevant to the analysis.\n",
    "The original dataset consists of 18,456,528 rows, including 449308 rows with missing data. After the selection, the number of rows are reduced to 7,428,781, approximately one-third of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "# Path to the dataset subfolder\n",
    "data_path = './dataset'\n",
    "\n",
    "# List all files in the subfolder\n",
    "# file_paths = [os.path.join(data_path, file) for file in os.listdir(data_path) if file.endswith('.csv.gz')]\n",
    "file_path = './dataset\\\\cityC_challengedata.csv.gz'\n",
    "city_name = os.path.basename(file_path).split('_')[0]\n",
    "# data = pd.read_csv(file_path, compression='gzip', nrows = 500000)\n",
    "data = pd.read_csv(file_path, compression='gzip')\n",
    "data = data[data['x'] != 999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days: 75\n",
      "Total number of participants: 20000\n",
      "Data size: 18007220\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of days: {len(data['d'].unique())}\")\n",
    "print(f\"Total number of participants: {len(data['uid'].unique())}\")\n",
    "print(f\"Data size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length=3):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for uid, group in data.groupby('uid'):\n",
    "        coords = group[['x', 'y']].values\n",
    "        for i in range(len(coords) - seq_length):\n",
    "            sequences.append(coords[i:i + seq_length])  # Input sequence\n",
    "            labels.append(coords[i + seq_length])  # Target value\n",
    "    return np.array(sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input sequences (X): (7368961, 3, 2)\n",
      "Shape of labels (y): (7368961, 2)\n",
      "Size of training data: 7428781\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "processed_data = data[data['d'] < 30]\n",
    "\n",
    "# Generate sequences and labels\n",
    "X, y = create_sequences(processed_data)\n",
    "\n",
    "# Display the shape of the prepared data\n",
    "print(f\"Shape of input sequences (X): {X.shape}\")\n",
    "print(f\"Shape of labels (y): {y.shape}\")\n",
    "print(f\"Size of training data: {len(processed_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m92112/92112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 4ms/step - loss: 66.9209 - val_loss: 48.0804\n",
      "Epoch 2/3\n",
      "\u001b[1m92112/92112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 4ms/step - loss: 47.5220 - val_loss: 47.5418\n",
      "Epoch 3/3\n",
      "\u001b[1m92112/92112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 5ms/step - loss: 47.3717 - val_loss: 47.8646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define LSTM model architecture\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dense(2)  # Output layer with 2 units for predicting (x, y)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Save the model for future use\n",
    "model.save('trajectory_predictor.h5')\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input sequences (X_test): (10250576, 3, 2)\n",
      "Shape of labels (y_test): (10250576, 2)\n"
     ]
    }
   ],
   "source": [
    "# Generate sequences and labels\n",
    "# X_test, y_test = create_sequences(data[(data['d'] > 30) & (data['d'] < 40)])\n",
    "X_test, y_test = create_sequences(data[(data['d'] > 30)])\n",
    "\n",
    "# Display the shape of the prepared data\n",
    "print(f\"Shape of input sequences (X_test): {X_test.shape}\")\n",
    "print(f\"Shape of labels (y_test): {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m320331/320331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m801s\u001b[0m 2ms/step\n",
      "Accuracy @ k: {1: 0.1342, 2: 0.3814, 3: 0.5409, 4: 0.6322, 5: 0.6933}\n"
     ]
    }
   ],
   "source": [
    "def compute_acc_at_k(model, X_test, y_test, k_values=[1, 2, 3, 4, 5]):\n",
    "    \"\"\"\n",
    "    Compute Accuracy at k (Acc@k).\n",
    "\n",
    "    Args:\n",
    "        model: Trained Keras model.\n",
    "        X_test: Test input data, shape (num_samples, 3, 2).\n",
    "        y_test: Ground truth labels, shape (num_samples, 2).\n",
    "        k_values: List of k values to compute Acc@k.\n",
    "\n",
    "    Returns:\n",
    "        acc_at_k: Dictionary with Acc@k for each k.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)  # Shape: (num_samples, 2)\n",
    "    acc_at_k = {k: 0 for k in k_values}\n",
    "    num_samples = X_test.shape[0]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        true_target = y_test[i]  # True target (x, y)\n",
    "        pred = predictions[i]  # Predicted location (x, y)\n",
    "        if true_target[0] == '-999': continue\n",
    "\n",
    "        # Compute Euclidean distances between prediction and true target\n",
    "        distance = np.linalg.norm(pred - true_target)\n",
    "\n",
    "        # Check if the distance qualifies as a \"hit\" within k thresholds\n",
    "        for k in k_values:\n",
    "            if distance <= k:  # Define threshold based on domain context\n",
    "                acc_at_k[k] += 1\n",
    "\n",
    "    # Normalize Acc@k\n",
    "    acc_at_k = {k: round(acc / num_samples, 4) for k, acc in acc_at_k.items()}\n",
    "    return acc_at_k\n",
    "\n",
    "# Example usage:\n",
    "acc_at_k = compute_acc_at_k(model, X_test, y_test)\n",
    "# acc_at_k = compute_acc_at_k(model, X, y)\n",
    "print(f\"Accuracy @ k: {acc_at_k}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "louvain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
